import requests, re, argparse, time, os

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}

# H√†m ƒë·ªçc danh s√°ch c√°c trang proxy t·ª´ file
def load_proxy_sites(file_path):
    try:
        with open(file_path) as f: return [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        print(f"üö® Kh√¥ng t√¨m th·∫•y file: {file_path}")
        return []

# H√†m qu√©t proxy t·ª´ trang web
def scrape_proxies(site):
    try:
        response = requests.get(site, headers=headers, timeout=7)
        if response.status_code == 200:
            proxies = re.findall(r"\d+\.\d+\.\d+\.\d+:\d+", response.text)
            print(f"\n{'='*50}\nƒêang qu√©t: {site}\nGET {response.status_code}\nS·ªë l∆∞·ª£ng proxy: {len(proxies)}\n{'='*50}")
            return proxies
        return print(f"\n{'='*50}\nƒêang qu√©t: {site}\nGET {response.status_code}\nTh·∫•t b·∫°i\n{'='*50}")
    except requests.exceptions.RequestException as e:
        return print(f"\n{'='*50}\nƒêang qu√©t: {site}\nL·ªói: {e}\n{'='*50}")

# H√†m l∆∞u proxy v√†o file
def save_proxies(proxies, filename="live.txt"):
    with open(filename, "w") as file: file.writelines(f"{proxy}\n" for proxy in proxies)
    return len(proxies)

# H√†m x√≥a b·∫£ng t·ªïng k·∫øt c≈©
def clear_screen(): os.system('cls' if os.name == 'nt' else 'clear')

# H√†m ch√≠nh qu√©t v√† x·ª≠ l√Ω proxy
def main():
    parser = argparse.ArgumentParser(description="Qu√©t proxy t·ª´ c√°c trang web.")
    parser.add_argument("-l", "--list", required=True, help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ch·ª©a danh s√°ch c√°c trang web proxy.")
    args = parser.parse_args()
    
    proxy_sites = load_proxy_sites(args.list)
    if not proxy_sites: return print("‚ö†Ô∏è Danh s√°ch proxy r·ªóng ho·∫∑c kh√¥ng h·ª£p l·ªá.")

    while True:
        clear_screen()  # X√≥a b·∫£ng t·ªïng k·∫øt c≈©
        all_proxies = set()  # L√†m m·ªõi danh s√°ch proxy m·ªói l·∫ßn qu√©t
        
        for site in proxy_sites:
            proxies = scrape_proxies(site)
            if proxies:
                all_proxies.update(proxies)  # C·∫≠p nh·∫≠t proxy m·ªõi
        
        if all_proxies:
            proxies_saved = save_proxies(all_proxies)
            print(f"üíæ ƒê√£ l∆∞u {proxies_saved} proxy v√†o live.txt.\n‚úÖ T·ªïng proxy t√¨m th·∫•y: {len(all_proxies)}")
        else:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y proxy h·ª£p l·ªá.")
        
        print(f"‚è≥ ƒê·ª£i 5 ph√∫t tr∆∞·ªõc khi qu√©t l·∫°i...")
        time.sleep(300)  # ƒê·ª£i 5 ph√∫t (300 gi√¢y)

if __name__ == "__main__": main()
